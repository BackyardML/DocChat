{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129144b1-95e3-47af-8f83-09eba6544a7f",
   "metadata": {},
   "source": [
    "# DocChat - Chat with your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9dd782-e0c5-4f35-aee0-f8a3a9bb8a41",
   "metadata": {},
   "source": [
    "This Jupyter notebook serves as a walkthrough for a machine learning and chatbot assignment, utilizing the Langchain framework and Large Language Models (LLMs). The assignment encompasses various stages, from importing dependencies and loading PDF documents to creating embeddings, conducting similarity searches, and setting up a conversational retriever with an LLM. The notebook guides you through these steps, illustrating how to extract valuable insights from text data, answer user queries, and build a functional chatbot using state-of-the-art natural language processing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9b0e2-1a1c-480a-a111-5c36bddb155c",
   "metadata": {},
   "source": [
    "This cell is where you import the necessary dependencies and libraries for your machine learning and chatbot project. These dependencies provide the essential tools and functionality that will be used in various parts of your Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30223e7f-b721-448e-b025-8033949d62ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone, Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531079ed-bdb5-43a8-a36c-1903e82434fe",
   "metadata": {},
   "source": [
    "This code configures and stores the OpenAI API key for secure access to OpenAI's services and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af367155-7194-4bea-860a-9eea10481f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c920fc2-a3af-4ad5-b4d0-d6784964e460",
   "metadata": {},
   "source": [
    "In this cell, you're setting up a pdf_loader to load PDF documents from a specified directory. The DirectoryLoader is configured to look for PDF files within the '../documents/' directory and its subdirectories (as indicated by the '**/*.pdf' pattern). This step is essential for accessing and processing the PDF documents in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c5be9e-4655-4939-bae7-ec32f6730a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 document(s) the provided directory\n",
      "Found 94878 characters in your document(s)\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = DirectoryLoader('../documents/', glob=\"**/*.pdf\")\n",
    "\n",
    "documents = []\n",
    "documents.extend(pdf_loader.load())\n",
    "\n",
    "num_docs = len(documents)\n",
    "num_chars = sum([len(document.page_content) for document in documents])\n",
    "\n",
    "print (f'Found {num_docs} document(s) the provided directory')\n",
    "print (f'Found {num_chars} characters in your document(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a5451-1b07-4f75-a5ca-4b87b9db6fd0",
   "metadata": {},
   "source": [
    "In this cell, you are employing the CharacterTextSplitter to segment the text content of your PDF documents into smaller, more manageable chunks. This text chunking process is crucial, especially when using large language models (LLMs) like GPT-3 or GPT-4. Here's why text chunking is necessary in relation to context length when working with LLMs:\n",
    "\n",
    "1. Managing Context Length: Large language models, such as GPT-3 or GPT-4, often have limitations on the maximum amount of text they can process in a single request. This is known as the \"context length.\" If your input text exceeds this limit, you'll need to truncate or split it into smaller portions to fit within the model's constraints.\n",
    "2. Preserving Contextual Understanding: To maintain the contextual understanding of the text, it's important to include enough context for the model to generate coherent responses. By breaking down the text into chunks with an appropriate overlap (as determined by chunk_size and chunk_overlap parameters), you ensure that the necessary context is retained in each chunk, and there is a degree of overlap to provide continuity between adjacent chunks.\n",
    "3. Enhancing Efficiency: Text chunking not only helps in managing context but also makes the processing of large documents more efficient. Smaller chunks are easier to work with, both in terms of memory and computational resources, which is essential when dealing with extensive datasets.\n",
    "4. Optimizing Model Usage: By chunking the text effectively, you can optimize your interactions with LLMs. It allows you to send manageable portions of text to the model for processing, reducing the likelihood of exceeding context length limitations and ensuring more accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da361b8-4c57-4f2f-95a7-b917cce4cb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1161, which is longer than the specified 1000\n",
      "Created a chunk of size 5132, which is longer than the specified 1000\n",
      "Created a chunk of size 1031, which is longer than the specified 1000\n",
      "Created a chunk of size 1172, which is longer than the specified 1000\n",
      "Created a chunk of size 1010, which is longer than the specified 1000\n",
      "Created a chunk of size 1398, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 1186, which is longer than the specified 1000\n",
      "Created a chunk of size 1232, which is longer than the specified 1000\n",
      "Created a chunk of size 1318, which is longer than the specified 1000\n",
      "Created a chunk of size 1158, which is longer than the specified 1000\n",
      "Created a chunk of size 1399, which is longer than the specified 1000\n",
      "Created a chunk of size 1111, which is longer than the specified 1000\n",
      "Created a chunk of size 1338, which is longer than the specified 1000\n",
      "Created a chunk of size 1479, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=40) #chunk overlap seems to work better\n",
    "documents = text_splitter.split_documents(documents)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d456f-b852-47df-9544-76a916b655ca",
   "metadata": {},
   "source": [
    "This cell sets up the creation of embeddings and a vector store for the text data from your documents. Here's a concise summary:\n",
    "\n",
    "1. Import Chroma and OpenAIEmbeddings modules.\n",
    "2. Create an embeddings instance for generating text embeddings.\n",
    "3. Initialize a vectorstore using Chroma from your chunked documents. This creates vector representations of the text, which are useful for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec349fd-2c4f-433b-b785-b17484eb3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8be87d-2366-456c-8e82-db69b3e93619",
   "metadata": {},
   "source": [
    "In this cell, you're conducting a similarity search using a provided text query:\n",
    "\n",
    "1. The variable text contains the query you want to search for, which is \"What makes Retentive Networks good?\"\n",
    "2. You use the vectorstore created earlier to perform a similarity search on the text query. This search aims to find documents or text chunks that are similar in meaning or context to the query text.\n",
    "\n",
    "This process allows you to retrieve documents or chunks of text that are semantically related to the query, which can be valuable for tasks such as information retrieval or recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca16d1a9-ecc1-499d-bb16-09c1babcd50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What makes Retentive Networks good?\"\n",
    "docs = vectorstore.similarity_search(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bd516-b9f3-4a8f-a781-f5e271c14a38",
   "metadata": {},
   "source": [
    "Next we're printing the content of the most similar document or text chunk that was returned by the similarity search conducted in the previous cell. This action allows you to see and inspect the content of the document that best matches the query you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29eee5a-e8c4-4d4f-8de5-a89f1a823a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Conclusion\n",
      "\n",
      "In this work, we propose retentive networks (RetNet) for sequence modeling, which enables various representations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers. The above advantages make RetNet an ideal successor to Transformers for large language models, especially considering the deployment benefits brought by the O(1) inference complexity. In the future, we would like to scale up RetNet in terms of model size [CDH+22] and training steps. Moreover, retention can efficiently work with structured prompting [HSD+22b] by compressing long-term memory. We will also use RetNet as the backbone architecture to train multimodal large language models [HSD+22a, HDW+23, PWD+23]. In addition, we are interested in deploying RetNet models on various edge devices, such as mobile phones.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7159833a-ba41-4f2e-983e-a118065a6126",
   "metadata": {},
   "source": [
    "In this cell, you are configuring a conversational retriever using the OpenAI language model (LLM) and the vector store for similarity-based retrieval:\n",
    "\n",
    "1. You import the necessary modules, including OpenAI from 'langchain.llms' to work with the language model.\n",
    "2. You create a retriever called retriever from the vectorstore with a specified search type (\"similarity\") and search parameters. This retriever is designed to find similar text based on vector representations.\n",
    "3. You establish a conversational retrieval chain, denoted as ´chatbot´, using the OpenAI LLM with a temperature of 0. The LLM will be used for answering questions or generating responses based on the retrieved content.\n",
    "\n",
    "This setup is fundamental for creating a chatbot or a system that can answer questions and engage in conversations by leveraging similarity-based content retrieval and the capabilities of the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee975b49-a460-4121-99e9-30784aabfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "chatbot = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aae980-886e-4a7c-becc-2c3d65ff120d",
   "metadata": {},
   "source": [
    "In this cell, you are simulating a chatbot interaction. Here's a succinct summary:\n",
    "\n",
    "1. chat_history is an empty list meant to store the chat history or conversation context.\n",
    "2. You define a query as \"What makes Retentive Networks good?\" which represents a user's question.\n",
    "3. Using the ´chatbot´ conversational retrieval chain, you provide the question and chat history as input to obtain a response from the chatbot.\n",
    "4. The response is stored in the result variable, and you extract the answer from it with result[\"answer\"].\n",
    "\n",
    "This code represents the interaction between a user's query and the chatbot, demonstrating how the chatbot generates responses based on the provided question and historical conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57eb2e5-76ae-4ba8-bb53-bddca6260d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Retentive Networks (RetNet) are good because they enable various representations, have significantly better inference efficiency, have favorable training parallelization, and have competitive performance compared to Transformers.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"What makes Retentive Networks good?\"\n",
    "result = chatbot({\"question\": query, \"chat_history\": chat_history})\n",
    "chat_history.append((query, result[\"answer\"]))\n",
    "result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
